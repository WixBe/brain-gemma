# ğŸ§  BrainGemma

> **AI-powered brain tumor diagnosis** â€” PathFoundation vision classifier + MedGemma clinical reasoning + React UI

---

## Architecture

```
brain-gemma/
â”œâ”€â”€ app/                          â† FastAPI backend
â”‚   â”œâ”€â”€ main.py                   â† CORS + routers
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ diagnose.py           â† POST /api/v1/diagnose  â† frontend calls this
â”‚   â”‚   â””â”€â”€ chat.py               â† POST /api/v1/chat      â† raw agent chat
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ agent.py              â† LangGraph agentic loop (PathFoundation â†’ MedGemma)
â”‚   â”‚   â”œâ”€â”€ vision.py             â† TumorClassifierTool (TF + PyTorch head)
â”‚   â”‚   â””â”€â”€ response_adapter.py  â† MedGemma JSON â†’ DiagnoseResponse schema
â”‚   â””â”€â”€ core/config.py           â† pydantic-settings (reads .env)
â”‚
â”œâ”€â”€ path_foundation_model/        â† Google PathFoundation TF SavedModel
â”œâ”€â”€ brain_tumor_path_foundation_head.pth  â† Fine-tuned 4-class PyTorch head
â”‚
â”œâ”€â”€ frontend/                     â† React (Vite) UI
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ api/mock.js           â† API client (real + mock mode)
â”‚   â”‚   â”œâ”€â”€ components/           â† Header, Hero, Dropzone, Results, Pipeline
â”‚   â”‚   â”œâ”€â”€ context/AppContext.jsx
â”‚   â”‚   â””â”€â”€ App.jsx
â”‚   â”œâ”€â”€ .env.local                â† VITE_API_BASE_URL, VITE_MOCK_MODE (gitignored)
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example                  â† Copy to .env and fill in values
â””â”€â”€ README.md
```

---

## Pipeline

```
Upload CT/MRI
    â”‚
    â–¼
POST /api/v1/diagnose
    â”‚
    â”œâ”€ Phase 1: PathFoundation TF â†’ embeddings â†’ PyTorch head
    â”‚           â†’ "GLIOMA (Confidence: 94.2%)"
    â”‚
    â””â”€ Phase 2: MedGemma synthesis
               image + Phase 1 result â†’ JSON report
               {primary_diagnosis, confidence, grade, location,
                differential_diagnosis, recommendations, triage_urgency}
                    â”‚
                    â–¼
              response_adapter.py  â†’  DiagnoseResponse
                    â”‚
                    â–¼
              React ResultsSection renders report
```

---

## Quick Start

### Prerequisites
- Python 3.10+
- Node.js 18+
- [LM Studio](https://lmstudio.ai/) running MedGemma locally  
  (or any OpenAI-compatible server at the URL in `.env`)

### 1. Backend Setup

```powershell
cd brain-gemma

# Create & activate virtual environment (always use venv)
python -m venv .venv
.venv\Scripts\activate          # Windows PowerShell
# source .venv/bin/activate     # macOS / Linux

# Install PyTorch with CUDA 12.4 (from PyTorch index)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124

# Install all other dependencies
pip install fastapi uvicorn pydantic pydantic-settings python-multipart `
            python-dotenv langchain langchain-core langchain-openai `
            langgraph langgraph-prebuilt pillow tensorflow tf_keras numpy

# Or install everything from requirements.txt (needs both indexes):
pip install -r requirements.txt --index-url https://download.pytorch.org/whl/cu124 `
            --extra-index-url https://pypi.org/simple

# Configure environment
cp .env.example .env
# Edit .env: set LLM_BASE_URL to your LM Studio IP

# Start the API server (using venv python)
.venv\Scripts\python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

API will be live at: **http://localhost:8000**  
Swagger docs: **http://localhost:8000/docs**

### 2. Frontend Setup

```bash
cd brain-gemma/frontend

# Install dependencies
npm install

# Configure environment (already set up â€” edit if needed)
# VITE_API_BASE_URL=http://localhost:8000
# VITE_MOCK_MODE=false

# Start development server
npm run dev
```

Frontend will be live at: **http://localhost:5173**

---

## API Endpoints

### `POST /api/v1/diagnose` â† Main endpoint (used by React frontend)

| Field | Type | Description |
|-------|------|-------------|
| `ct` | `File[]` | CT scan files (optional) |
| `mri` | `File[]` | MRI scan files (optional) |
| `context` | `string` | Optional clinical notes |

**Response:** `DiagnoseResponse`
```json
{
  "diagnosis": "High-Grade Glioma",
  "tumor_type": "Glioblastoma Multiforme (GBM)",
  "grade": "WHO Grade IV",
  "confidence": 94,
  "location": "Left Temporal Lobe",
  "modalities_used": ["MRI"],
  "triage": "URGENT",
  "findings": "...",
  "recommendations": ["..."],
  "differential": [{"label": "Glioma", "probability": 94}],
  "inference_ms": 1240
}
```

### `POST /api/v1/chat` â† Raw agent chat (for testing/debug)

| Field | Type | Description |
|-------|------|-------------|
| `message` | `string` | Text query |
| `image` | `File` | Optional scan image |

### `GET /health` or `GET /api/v1/health`
Returns model load status and LLM connectivity.

---

## Configuration (`.env`)

| Variable | Default | Description |
|----------|---------|-------------|
| `LLM_BASE_URL` | `http://localhost:1234/v1` | LM Studio / OpenAI-compat server |
| `LLM_MODEL` | `medgemma-1.5-4b-it` | Model name |
| `LLM_API_KEY` | `lm-studio` | API key (any string for LM Studio) |
| `UPLOAD_DIR` | `uploads` | Temp directory for uploaded scans |
| `ENV` | `development` | `development` = open CORS; `production` = locked |
| `PORT` | `8000` | Server port |

---

## Supported File Types

`.jpg` Â· `.jpeg` Â· `.png` Â· `.dcm` Â· `.nii` Â· `.gz` Â· `.bmp` Â· `.tiff`

---

## Tumor Classes

| PathFoundation class | Display | Triage |
|---------------------|---------|--------|
| `glioma` | High-Grade Glioma (GBM) | ğŸ”´ URGENT |
| `meningioma` | Meningioma | ğŸŸ¡ SOON |
| `pituitary` | Pituitary Adenoma | ğŸŸ¡ SOON |
| `notumor` | No Tumor Detected | ğŸŸ¢ ROUTINE |

---

## âš ï¸ Disclaimer

This tool is for **research and demonstration purposes only**.  
It is **NOT approved for clinical use**. Always consult a qualified medical professional.

---

## License

MIT
